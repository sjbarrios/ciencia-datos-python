{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universidad Galileo\n",
    "\n",
    "Ciencia de Datos en Python\n",
    "\n",
    "PAPD - Sección V\n",
    "\n",
    "Sergio José Barrios Martínez\n",
    "\n",
    "Carnet No. 19012765\n",
    "\n",
    "# Ejercicios Matrices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructores de matrices\n",
    "\n",
    "NumPy provee diversas funciones para crear matrices , algunas de las cuales ya vimos cuando estudiamos vectores , la diferencia consiste en que ahora ya no usamos un número para indicar el tamaño de un vector, si no una **tupla** de 2 elementos: (m,n) . Algunas de las funciones  que aplican tanto a vectores como a matrices son:\n",
    "\n",
    "* np.array: crear una matriz a partir de una lista de listas: cada fila es una sublista\n",
    "* np.zeros: crear una matriz de ceros\n",
    "* np.ones: crear una matriz de unos\n",
    "* np.empty: crear una matriz sin importarnos sus valores\n",
    "* np.full: crear una matriz  con cierto valor\n",
    "* np.copy: crea un clon o copia de cierta matriz\n",
    "\n",
    "Algunas funciones específicas de matrices son:\n",
    "\n",
    "* np.matrix: resultado casi identico a la función mas general np.array ,pero posee algunas propiedades adicionales específicas de listas, por ejemplo notación sencilla para inversas de matrices.\n",
    "* np.eye: crear una matriz con 1s en su diagonal principal y ceros en el resto\n",
    "* np.identity : crear una matriz identidad\n",
    "\n",
    "Algunas funciones que solo aplican a vectores y no a matrices son:\n",
    "\n",
    "* np.arange\n",
    "* np.linspace\n",
    "\n",
    "Existen otras pero estas son posiblemente las mas comunes. Puedes consultar las otras disponibles en: https://docs.scipy.org/doc/numpy/reference/routines.array-creation.html\n",
    "\n",
    "\n",
    "## Ejercicio No. 1\n",
    "\n",
    "**Ejercicio**: Investigar y ejemplificar diferencias entre np.array y np.matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se mencionó anteriormente, **numpy.matrix** tiene propiedades específicas para listas, enfocadas para el uso de matrices, es decir, se limita a arreglos bi-dimensionales. **numpy.array** es más general, pudiéndose extender a arreglos multidimensionales. Desde este punto de vista, *numpy.matrix* es un caso específico de *numpy.array*, (de hecho *numpy.matrix* es una sub-clase de **ndarrays** y tiene los mismos métodos y características de esta clase).\n",
    "\n",
    "\n",
    "Sin embargo, internamente, sí se hace una distinción de qué tipo de arreglo son. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]] <class 'numpy.ndarray'> 2\n",
      "[[1 2 3]] <class 'numpy.matrix'> 2\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2,3]]) # Para formar una matriz, se especifica una lista de listas\n",
    "print(a,type(a),np.ndim(a))\n",
    "b = np.matrix([1,2,3]) # Para formar una matrix, únicamente se especifican las listas\n",
    "print(b,type(b),np.ndim(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanto los *nd.arrays* como las *np.matrix* comparten algunas operaciones aritméticas, e incluso matriciales, por ejemplo:\n",
    "* Operador +,-,T,I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]]\n",
      "[[1]\n",
      " [2]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "transpuesta_a = a.T\n",
    "transpuesta_b = b.T\n",
    "print (transpuesta_a)\n",
    "print (transpuesta_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, hay propiedades que solamente se encuentran en el objeto tipo *matrix*, por ejemplo, la propiedad **\"I\"** para retornar la matriz inversa y la propiedad **\"H\"** para retornar la inversa conjugada, algo que no tienen los np.arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07142857]\n",
      " [0.14285714]\n",
      " [0.21428571]]\n",
      "[[1]\n",
      " [2]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "print (b.I)\n",
    "print (b.H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El operador de potenciación ****** también se comporta diferente. En el caso de los np.arrays, aplicar el operador $**$2 devuelve cada elemento elevado al cuadrado, mientras que en matrices, devuelve la matrix multiplicada por ella misma. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  4]\n",
      " [ 9 16]]\n",
      "[[ 7 10]\n",
      " [15 22]]\n"
     ]
    }
   ],
   "source": [
    "c_array = np.array([[1,2],[3,4]])\n",
    "print(c_array**2) # Potenciación 2 elemento a elemento\n",
    "c_matriz = np.matrix([[1,2],[3,4]]) # Multiplicación Matricial c * c\n",
    "print(c_matriz**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplicación Matricial\n",
    "\n",
    "**Ejemplo en DS** : en inteligencia artificial y ML en la sub-rama \"reinforcement learning\" la \"ecuacion de bellman\" puede aplicarse de manera vectorizada a traves del operar vectores, matrices y escalares en una sola expresion.\n",
    "\n",
    "<img src=\"https://rebornhugo.github.io/assets/images/post_images/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/bellman%20equation2.PNG\">\n",
    "\n",
    "* n = numero de estados del sistema.\n",
    "* V(s) = vector que representa el valor esperado para cierto estado\n",
    "* R = recompensa inmediata percibida por el agente al salir de cierto estado.(vector)\n",
    "* P = matriz de transicion de la cadena de Markov sub-yacente.(matriz)\n",
    "* γ = factor de descuento de recompensas futuras(escalar)\n",
    "\n",
    "\n",
    "## Ejercicio No. 2\n",
    "Calcular V(s) para el siguiente sistema aplicando la ecuación de bellman de manera vectorizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.array([0,0,0]) # valor inicial de V(s)\n",
    "R = np.array([10,2,5]) # vector de recompensas\n",
    "P = np.array([[0.5,0.25,0.25],\n",
    "              [0.2,0.40,0.40],\n",
    "              [0.80,0.10,0.10]])  # matriz de transición\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.  2.  5.]\n"
     ]
    }
   ],
   "source": [
    "# Cálculo de V(s) de manera vectorizada\n",
    "V = R + np.matmul(gamma*P,V) # Función \"matmul\" para general la multiplicación matricial\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio No. 2 \n",
    "\n",
    "Implementar en una funcion neural_network(X) la red neuronal definida por la siguiente arquitectura:\n",
    "\n",
    "<img src=\"http://i.imgur.com/UNlffE1.png\">\n",
    "\n",
    "Podemos validar si fue correctamente implementada si usamos como entrada el vector x=[1,1] . Debemos obtener el resultado mostrado en la imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definirán dos funciones, una para aceptar un vector \"X\" de 2x1 (neural_network_v),\n",
    "# y otra para aceptar un vector \"X\" de 1x2 (neutral_network_h) para verificar cómo\n",
    "# cambia la operatoria. Para implementación real, únicamente necesitaríamos una de las\n",
    "# dos, y valiéndonos de la función SHAPE adecuar (por medio de TRANSPOSE) la forma de\n",
    "# la entrada \"X\". Para este ejercicio se implementan ambas funciones para ser más explícitos \n",
    "# en la operatoria que se requeriría en cada caso.\n",
    "\n",
    "# Función para la Red Neuronal, con X una capa de entrada de (2x1)\n",
    "def neural_network_v(X):\n",
    "    \n",
    "    # Matriz de Pesos para la Capa de Entrada \"X\" (3x2)\n",
    "    H1_W = np.array([[0.712,0.112],\n",
    "                     [0.355,0.855],\n",
    "                     [0.268,0.468]])\n",
    "\n",
    "    # Bias sobre la capa oculta (deducida de la información) (3x1)\n",
    "    BIAS_H1 = np.array([[-0.13],\n",
    "                        [-0.44],\n",
    "                        [-0.06]])\n",
    "    \n",
    "    # Matriz de Pesos para la Capa de Salida \"OL\"\n",
    "    OL_W = np.array([[0.116,0.329,0.708]])\n",
    "    \n",
    "    # Bias sobre la capa de salida (deducida de la información) (1x1)\n",
    "    BIAS_OL = np.array([-0.12]),\n",
    "\n",
    "    H1 = np.matmul(H1_W,X)\n",
    "    print(\"Capa Oculta Sin Bias:\")\n",
    "    print(H1)\n",
    "    H1+=BIAS_H1\n",
    "    print(\"Capa Oculta Con Bias:\")\n",
    "    print(H1)\n",
    "    OL = np.matmul(OL_W,H1)\n",
    "    print(\"Capa Salida Sin Bias:\")\n",
    "    print(OL)\n",
    "    OL+=BIAS_OL\n",
    "    print(\"Capa Salida Con Bias:\")\n",
    "    print(OL)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "# Función para la Red Neuronal, con X una capa de entrada de (1x2)\n",
    "def neural_network_h(X):\n",
    "    \n",
    "    # Matriz de Pesos para la Capa de Entrada \"X\" (3x2)\n",
    "    H1_W = np.array([[0.712,0.355,0.268],\n",
    "                     [0.112,0.855,0.468]])\n",
    "\n",
    "    # Bias sobre la capa oculta (deducida de la información) (3x1)\n",
    "    BIAS_H1 = np.array([-0.13,-0.44,-0.06])\n",
    "    \n",
    "    # Matriz de Pesos para la Capa de Salida \"OL\"\n",
    "    OL_W = np.array([[0.116],[0.329],[0.708]])\n",
    "    \n",
    "    # Bias sobre la capa de salida (deducida de la información) (1x1)\n",
    "    BIAS_OL = np.array(-0.12),\n",
    "\n",
    "    H1 = np.matmul(X,H1_W)\n",
    "    print(\"Capa Oculta Sin Bias:\")\n",
    "    print(H1)\n",
    "    H1+=BIAS_H1\n",
    "    print(\"Capa Oculta Con Bias:\")\n",
    "    print(H1)\n",
    "    OL = np.matmul(H1,OL_W)\n",
    "    print(\"Capa Salida Sin Bias:\")\n",
    "    print(OL)\n",
    "    OL+=BIAS_OL\n",
    "    print(\"Capa Salida Con Bias:\")\n",
    "    print(OL)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capa Oculta Sin Bias:\n",
      "[[0.824]\n",
      " [1.21 ]\n",
      " [0.736]]\n",
      "Capa Oculta Con Bias:\n",
      "[[0.694]\n",
      " [0.77 ]\n",
      " [0.676]]\n",
      "Capa Salida Sin Bias:\n",
      "[[0.812442]]\n",
      "Capa Salida Con Bias:\n",
      "[[0.692442]]\n"
     ]
    }
   ],
   "source": [
    "#Prueba para vector \"X\" de 2x1\n",
    "X =  np.array([[1],[1]])\n",
    "neural_network_v(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capa Oculta Sin Bias:\n",
      "[0.824 1.21  0.736]\n",
      "Capa Oculta Con Bias:\n",
      "[0.694 0.77  0.676]\n",
      "Capa Salida Sin Bias:\n",
      "[0.812442]\n",
      "Capa Salida Con Bias:\n",
      "[0.692442]\n"
     ]
    }
   ],
   "source": [
    "#Prueba para vector \"X\" de 1x2\n",
    "X =  np.array([1,1])\n",
    "neural_network_h(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos la implementacion correcta, cambiar la funcion de activacion de la capa de salida por la funcion de activacion ReLu(https://en.wikipedia.org/wiki/Rectifier_(neural_networks)):\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*DfMRHwxY1gyyDmrIAd-gjQ.png\">\n",
    "\n",
    "Luego evaluar la red neuronal sobre la matriz de datos X(de manera vectorizada):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de Activación ReLu\n",
    "# Se utiliza piecewise para definirla\n",
    "def ReLu(x):\n",
    "    k=np.piecewise(x,[x<0,x>=0],[lambda x: 0,lambda x: x])\n",
    "    return(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para la Red Neuronal, con X una capa de entrada de (1x2)\n",
    "def neural_network(X):\n",
    "    \n",
    "    # Matriz de Pesos para la Capa de Entrada \"X\" (3x2)\n",
    "    H1_W = np.array([[0.712,0.355,0.268],\n",
    "                     [0.112,0.855,0.468]])\n",
    "\n",
    "    # Bias sobre la capa oculta (deducida de la información) (3x1)\n",
    "    BIAS_H1 = np.array([-0.13,-0.44,-0.06])\n",
    "    \n",
    "    # Matriz de Pesos para la Capa de Salida \"OL\"\n",
    "    OL_W = np.array([[0.116],[0.329],[0.708]])\n",
    "    \n",
    "    # Bias sobre la capa de salida (deducida de la información) (1x1)\n",
    "    BIAS_OL = np.array(-0.12),\n",
    "\n",
    "    H1 = np.matmul(X,H1_W)\n",
    "    print(\"Capa Oculta Sin Bias:\")\n",
    "    print(H1)\n",
    "    H1+=BIAS_H1\n",
    "    print(\"Capa Oculta Con Bias:\")\n",
    "    print(H1)\n",
    "    H1 = ReLu(H1)\n",
    "    print(\"Capa Oculta Con Bias y ReLu:\")\n",
    "    print(H1)   \n",
    "    OL = np.matmul(H1,OL_W)\n",
    "    print(\"Capa Salida Sin Bias:\")\n",
    "    print(OL)\n",
    "    OL+=BIAS_OL\n",
    "    print(\"Capa Salida Con Bias:\")\n",
    "    print(OL)\n",
    "    OL = ReLu(OL)\n",
    "    print(\"Capa Salida Con Bias y ReLu:\")\n",
    "    print(OL)   \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capa Oculta Sin Bias:\n",
      "[0.824 1.21  0.736]\n",
      "Capa Oculta Con Bias:\n",
      "[0.694 0.77  0.676]\n",
      "Capa Oculta Con Bias y ReLu:\n",
      "[0.694 0.77  0.676]\n",
      "Capa Salida Sin Bias:\n",
      "[0.812442]\n",
      "Capa Salida Con Bias:\n",
      "[0.692442]\n",
      "Capa Salida Con Bias y ReLu:\n",
      "[0.692442]\n"
     ]
    }
   ],
   "source": [
    "#Prueba para vector \"X\" de 1x2\n",
    "X =  np.array([1,1])\n",
    "neural_network(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, en este caso específico con X=[1,1] los resultados no cambian, ya que cada salida (en la capa oculta y la capa de salida propiamente) es mayor que cero para esta X, y la función de activación ReLu la deja \"pasar\" sin cambios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
